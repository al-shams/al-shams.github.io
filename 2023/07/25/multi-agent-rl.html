<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Ali Shamsadini | Multi-Agent Deep Reinforcement Learning-Based Framework
        for Resource Management in Self-Organizing Networks</title>
    <!-- Bootstrap core CSS -->
    <link href="/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.svg?">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
    <title>Multi-Agent Deep Reinforcement Learning-Based Framework
        for Resource Management in Self-Organizing Networks | Ali Shamsadini</title>
    <meta name="generator" content="Jekyll v4.0.0"/>
    <meta property="og:title"
          content="Multi-Agent Deep Reinforcement Learning-Based Framework for Resource Management in Self-Organizing Networks"/>
    <meta property="og:locale" content="en_US"/>
    <meta name="description"
          content="Reinforcement Learning (RL) has demonstrated its capabilities in designing effective logic within controlled environments. However, exploring its applications in more complex domains, such as telecom networks, presents exciting possibilities."/>
    <meta property="og:description"
          content="Reinforcement Learning (RL) has demonstrated its capabilities in designing effective logic within controlled environments. However, exploring its applications in more complex domains, such as telecom networks, presents exciting possibilities."/>
    <link rel="canonical" href="https://al-shams.github.io/2023/07/25/multi-agent-rl.html"/>
    <meta property="og:url" content="https://al-shams.github.io/2023/07/25/multi-agent-rl.html"/>
    <meta property="og:site_name" content="Ali Shamsadini"/>
    <meta property="og:type" content="article"/>
    <meta property="article:published_time" content="2023-07-25T00:00:00-05:00"/>
    <script type="application/ld+json">
        {
            "headline": "Multi-Agent Deep Reinforcement Learning-Based Framework for Resource Management in Self-Organizing Networks",
            "dateModified": "2023-07-26T00:00:00-05:00",
            "datePublished": "2023-07-26T00:00:00-05:00",
            "url": "https://al-shams.github.io/2023/07/25/multi-agent-rl.html",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "https://al-shams.github.io/2023/07/25/multi-agent-rl.html"
            },
            "description": "Reinforcement Learning (RL) has demonstrated its capabilities in designing effective logic within controlled environments. However, exploring its applications in more complex domains, such as telecom networks, presents exciting possibilities.",
            "@type": "BlogPosting",
            "@context": "https://schema.org"
        }</script>
    <!-- End Jekyll SEO tag -->
</head>
<body>
<!-- Navigation -->
<nav class="navbar fixed-top navbar-expand-lg navbar-light fixed-top bg-bluelight">
    <div class="container">
        <a class="navbar-brand" href="/index.html">
            Ali Shamsadini
        </a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
                data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
                aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link  " href="/"> <strong>Home</strong></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link  " href="/education.html"> <strong>Education</strong></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link  " href="/publications.html"> <strong>Publications</strong></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link  " href="/experiences.html"> <strong>Experiences</strong></a>
                </li>
                <li class="nav-item">
                    <a class="nav-link  " href="/blog.html"> <strong>Blog</strong></a>
                </li>
            </ul>
        </div>
    </div>
</nav>
<!-- Header Slide -->
<!-- Page Content -->
<div class="container">
    <h1 class="my-4">
        Multi-Agent Deep Reinforcement Learning-Based Framework
        for Resource Management in Self-Organizing Networks
    </h1>
    <ol class="breadcrumb">
        <li class="breadcrumb-item">
            <a href="/index.html">Home</a>
        </li>
        <li class="breadcrumb-item active"><a href="/blog.html">Blog</a></li>
        <li class="breadcrumb-item active">Multi-Agent Deep Reinforcement Learning-Based Framework
            for Resource Management in Self-Organizing Networks
        </li>
    </ol>
    <hr>
    <div class="row justify-content-lg-center">
        <!-- Blog Entries Column -->
        <div class="col-lg-10 col-12">
            <!-- Blog Post -->
            <div class="my-3 text-muted">
                July 25, 2023
            </div>
            <div class="card-text"><p>The following report was compiled as part of my comprehensive research on applying
                reinforcement learning solutions in telecom networks. To obtain the complete report,
                please contact me via email.</p>
                <hr/>
                <h4>Abstract</h4>
                <p>
                    In recent decades, mobile networks have expanded to provide coverage to as many people as
                    possible. In this regard, urban areas with higher user density have better service coverage than
                    less populated areas, mainly in the vicinity of cities. However, as continuous access to the
                    services of these networks has become a necessity today, mobile phone operators are expanding their
                    networks to cover more users in remote areas. Even in areas with sufficient coverage, service
                    quality can diminish during peak times due to network congestion and the limited resources of mobile
                    phone operators. One possible solution is to increase the density of cells to boost the overall
                    network capacity in such situations.
                    While this short-term oversupply of resources can improve service quality, it results in long-term
                    costs economically (in terms of updating new base stations and enhancing the network core) and
                    environmentally (related to providing energy to new base stations) for mobile phone operators.
                    Therefore, solutions should be sought to reduce the costs associated with deploying these resources
                    while providing suitable service quality to users. These solutions should consider criteria such as
                    dynamic demand for network resources, traffic patterns, and, notably, the spatiotemporal
                    characteristics of mobile phone users.
                    Within the life cycle of radio networks, these issues fall under the category of self-optimization
                    problems. They encompass a range of approaches and mechanisms to optimize key network parameters in
                    real-time, using data received from network cells. One of the most widely recognized challenges for
                    which practical solutions have been implemented or are in progress by leading operators worldwide is
                    optimizing network cell coverage and capacity. This problem involves numerous adjustable parameters
                    and complex dependencies that cannot be effectively addressed using traditional AI approaches.
                    Recent research and practical efforts are showing promise in solving such problems with extensive
                    state spaces using deep reinforcement learning approaches.
                </p>
                <h6 id="index-terms"><em>Index Terms</em>: Multi-agent Systems, Deep Reinforcement Learning,
                    Self-organizing Networks</h6>
                <h4>&#8544. INTRODUCTION</h4>
                <p>
                    Self-organized networks were initially introduced by the 3GPP standardization organization to
                    streamlining the planning, configuration, management, optimization, and troubleshooting of
                    radio access networks. This concept has evolved through various versions of the LTE and NGMN
                    standards. It encompasses a suite of self-configuration, self-optimization, and self-healing
                    solutions. Fig. 1 illustrates the placement of these three solutions within the lifecycle of radio
                    networks.
                <div class="row justify-content-center">
                    <div class="figure col col-sm-6 col-10 align-self-center my-4 img">
                        <img class="rounded" alt="Life-Cycle of Radio Network"
                             src="/assets/images/life-cycle-of-radio-network.svg"/>
                        <div class="figure-caption text-center text-dark">Fig. 1. Life-Cycle of Radio Network
                        </div>
                    </div>
                </div>
                <ul>
                    <li><strong>Self-configuration:</strong> The functions within this solution aim to automate the
                        introduction of new services with minimal human intervention and are integrated into the
                        planning and deployment phases of radio networks.
                    </li>
                    <li><strong>Self-healing:</strong> Elements within radio networks are susceptible to errors and
                        failures. Self-healing functions concentrate on the maintenance phase of these elements. For
                        instance, in the event of a network cell failure, the services of users within the affected cell
                        are automatically transitioned to nearby cells with available capacity.
                    </li>
                    <li><strong>Self-optimization:</strong> After designing and deploying the network elements,
                        configuring the relevant parameters, establishing connections to the network core, and
                        ultimately transitioning to operational mode, it is essential to enhance the efficiency of these
                        elements continuously. This improvement is performed in real-time based on data received from
                        network cells. In this proposed design, our primary emphasis will be on the functions within
                        this section.
                    </li>
                </ul>
                </p>
                <h4>&#8545. SUCCESSFUL RL USE CASES - SWISSCOM</h4>
                <p>
                    As Switzerland's largest telecommunications company, providing a wide array of communication
                    services, Swisscom confronts the challenge of curbing energy emissions in the existing low-band
                    layers. This energy reduction must be substantial enough to create space to establish a new low-band
                    layer. Stringent Swiss government regulations govern the effective radiation power of mobile
                    networks, on the one hand, and the inadequacy of power in the new low-band layer and the
                    coverage mismatch with the existing layer, on the other hand, have compelled the company to employ
                    reinforcement learning approaches to address this challenge. As illustrated in Fig. 2, they
                    initially optimized the ERP parameter and subsequently addressed the RET parameter in two separate
                    steps.
                <div class="row justify-content-center">
                    <div class="figure col col-sm-4 col-10 align-self-center my-4 img">
                        <img class="rounded" alt="Live Networks Using Simulators and Emulators as Digital Twins"
                             src="/assets/images/live-networks-using-simulators-and-emulators-as-digital-twins.svg"/>
                        <div class="figure-caption text-center text-dark">Fig. 2. Live Networks Using Simulators and
                            Emulators as Digital Twins
                        </div>
                    </div>
                </div>
                <ul>
                    <li><strong>Effective Radiated Power:</strong> Minimizing the ERP parameter to the greatest extent
                        possible while preserving coverage and maintaining the quality of service to users by applying
                        reinforcement learning techniques and utilizing a network simulator, acting as a digital
                        counterpart.
                    </li>
                    <li><strong>Remote Electrical Tilt:</strong> Enhancing the RET parameter through a simulator-based
                        optimization approach.
                    </li>
                </ul>
                The experiment focused on optimizing the transmission power of the uplink connection and adjusting the
                RET parameter in the Ticino region of Switzerland. This region comprises 163 fourth-generation cells
                operating in the 800 MHz band, with 100 cells selected to optimize uplink transmission power and
                subsequent RET parameter optimization. According to reports, the emulation of network behavior following
                power adjustments was remarkably precise, eliminating the need for repeated online interactions with the
                network. Instead, the final optimized values were acquired exclusively through interactions with the
                digital counterpart. These values were then directly implemented and integrated into the network.
                Subsequently, RET parameter optimization was implemented within the network, with the ultimate
                modifications displayed in Fig. 3.
                As indicated in Fig. 3, in areas where network antennas consume the maximum power, the electrical
                tilt of the antenna is at its minimum, and vice versa. As a result of this testing, uplink transmission
                power decreased by 10%, while uplink connection throughput increased by 12%. Additionally, after a
                single iteration of optimization and network interaction, cumulative transmission power decreased by
                20%, and uplink connection throughput increased by 5.5%. This reduction in the ERP parameter translates
                to a 4.3% decrease in base station power consumption. Swisscom's experience is another example of
                applying reinforcement learning approaches to learn, optimize, and ultimately automate complex processes
                without human intervention.
                <div class="row justify-content-center">
                    <div class="figure col col-sm-7 col-10 align-self-center my-4 img">
                        <img class="rounded" alt="Power and RET Changes in the Ticino Region of Switzerland"
                             src="/assets/images/power-and-RET-changes-in-the-Ticino-area-of-Switzerland.svg"/>
                        <div class="figure-caption text-center text-dark">Fig. 3. Power and RET Changes in the Ticino
                            Region of Switzerland
                        </div>
                    </div>
                </div>
                </p>
            </div>
            <hr>
            <div class="categories">
                <span class="badge badge-info">Multi-agent Systems</span>
                <span class="badge badge-info">Deep Reinforcement Learning</span>
                <span class="badge badge-info">Self-organizing Networks</span>
            </div>
            <hr>
            <div class="more-links mt-3">
                <h5 class="mb-1">Want to read more?</h5>
                Check out these pages:
                <ul class="mt-2">
                    <li>
                        <a href="https://www.ericsson.com/en/blog/2022/3/reinforcement-learning-solutions">
                            Bringing RL Solutions to Action in Telecom Networks</a>
                    </li>
                    <li>
                        <a href="https://www.ericsson.com/en/reports-and-papers/mobility-report/articles/reinforcement-learning">
                            Applying AI in Telecoms</a>
                    </li>
                    <li>
                        <a href="https://www.ericsson.com/en/blog/2023/11/reinforcement-learning">
                            Demystifying Online and offline RL</a>
                    </li>
                </ul>
            </div>
        </div>
    </div>
</div>
<!-- /.container -->
<!-- Footer -->
<footer class="py-3 bg-bluelight">
    <div class="container">
        <p class="m-0 text-center text-black txt-small">Copyright &copy; 2024, Ali Shamsadini</p>
    </div>
    <!-- /.container -->
</footer>
<!-- Bootstrap core JavaScript -->
<script src="/vendor/jquery/jquery.min.js"></script>
<script src="/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="/assets/js/custom.js"></script>
</body>
</html>
